\chapter{Introduction}  
Hidden Markov Modelle bieten die möglichkeit komplexe Systeme zu modellieren. Sie wurden im frühen zwanzigsten Jahrhundert durch Andrey Markov eingeführt \cite*{rabiner}. Zunächst war die Forschung an Hidden Markov Modellen primär theoretischer Natur, bis in den späten 1960er bis frühen 1970er Jahren effiziente Methoden für die Parameterestimation durch Leonard E. Baum und seine Kollegen entwickelt wurden. Seit jeher finden Hidden Markov Modelle praktische Anwendungen in einer Vielzahl an Wissenschaftlichen Disziplinen. Hidden Markov Modelle sind beliebt, da sie unter anderem sehr flexibel und einfach zu implementieren sind \cite*{HmmReview} und mittleerweile aus den Gebieten der Spracherkennung \cite*{ApplicationSpeechRecognition} und Bioinformatik \cite*{ApplicationComputationalBiology} nicht mehr wegzudenken. Eine der prominentesten Anwendungen von Hidden Markov Modellen ist der Google Pagerank Algorithmus, welcher die Relevanz von Webseite in Relation zu einer Suchanfrage ermittelt \cite*{ApplicationPageRank}.

\subsubsection*{Motivation}
Wie gut ein trainiertes Hidden Markov Model einen Prozess modelliert ist abhängig von der Qualität der Parameter des Modells. Die am häufigsten verwendete Methode für das ermitteln der Parameter ist der Baum-Welch Algorithmus. Dieser ist jedoch anfällig dafür in lokalen Optima stecken zu bleiben. Daher wurden in den letzten Jahren zahlreiche alternative Trainingsverfahren für HMMs entwickelt. Viele davon sind sogenannte Metaheuristiken, stochastische Verfahren die durch natürliche Prozesse wie Evolution inspiriert sind. Trotz dieser großen Auswahl alternativer Trainingsverfahren besteht die predominante Trainingsstrategie weiterhin darin, den Baum-Welch Algorithmus mehrmals auf verschiedene initiale Parametern anzuwenden und das beste resultierende Modell zu wählen. So lautet es zumindestens in der Dokumentation zu \textit{hmmlearn}, einer der beliebtesten HMM python libraries. Es stellt sich also die Frage warum keine der alternativen Trainingsverfahren so großen Anklang wie der Baum-Welch Algorithmus gefunden hat.

\subsubsection*{Struktur}
Ich werde zunächst die Grundlagen auslegen, dazu gehören eine Übersicht zum Aufbau und stochastische Methoden für Hidden Markov Modelle. Danach widmen wir uns allgemeiner dem Gebiet der Optimierung und gehe auf Metaheuristische Algorithmen ein sowie derer Kritik. Danach werden genetische Algorithmen erklärt. Im zweiten Kapitel werden die Konzepte aus den Grundlagen kombiniert und es wird darauf eingegangen wie man Hidden Markov Modelle mit einem genetischen Algorithmus kombinieren kann und worauf man dabei achten sollte. Des Weiteren wird ein Überblick verschafft über die bisher unternommenen metaheuristischen Trainingsansätze für HMMs. Im dritten Kapitel gehe ich auf die Implementation eines Frameworks für genetische Algorithmen und Hidden Markov Modelle ein. Danach werden drei Ausgewählte Ansätze die Hidden Markov Modelle mit einem genetischen Algorithmus trainieren evaluiert. Die Arbeit konkludiert mit einem Fazit über die feasibility von genetischen Algorithmen für das training von HMMs, so wie einer Diskussion über den Zustand des Feldes der Metaheuristischen Optimierung und gibt einen Ausblick über zukünftige Forschung. 
